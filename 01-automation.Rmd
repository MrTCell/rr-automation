
# Introduction to automation

> "Reproduciibilty is actually all about being as lazy as possible!", <br/>
> -- Hadley Wickham (via
> [Twitter](https://twitter.com/hadleywickham/status/598532170160873472),
> 2015-05-03)


## Disclaimer

Depending on your previous experience with knitr, it might seem too advanced. If
you have never used knitr before this is a good opportunity to learn efficient
practices right away.

Throughout, I will try to give two options: one that has a low activiation
energy and that you'll be able to implement rapidly; and a better way of doing
things that may require you to learn some other tools and techniques.

There is nothing wrong with having your analysis entirely written as script
without any functions, and it's certainly orders of magnitude better than not
incorporating any reproducibility in your analysis. The goal of this session is
to teach you some tips and tricks that will make your life easier with knitr if
you are using it in the context of a research paper that involves multiple
sources of raw data that are combined to create multiple intermediate datasets
that are themselves combined to generate figures and tables.

You certainly don't need to adopt all of these practices at once. Start
small. Maybe with only one dataset, one figure first, and later learn how to use
Make, and then Travis. None of these tools are exceedingly hard to learn or to
grasp, but trying to learn everything at once might feel overwhelming. However,
in the process of learning these tools, you will learn skills that you will be
able to translate into other components of your research. For instance, if you
are interested in learning how to write a package in R, many of the advice and
tools included in this lecture will greatly help you to get started. If you
start using Make and Travis, you will learn some basics of UNIX and Linux which
are general useful skills to have, if you are interested in analyzing large
datasets (e.g., set up runs on an HPC cluster).

## The issue(s)

knitr allows you to mix code and prose, which is wonderful and very powerful,
but can be difficult to manage if you don't have a good plan to get
organized. The challenge with working in this reproducible framework is that you
end up developing your analysis at the same time as you are writting your
manuscript and refining your ideas, adjusting the aim of your paper, deciding on
the data you are going to include, etc. It's therefore important that you have a
modular framework in place where each section of your analysis can be
self-contained so you don't depend on a linear script that will not reflect the
complexity of your analysis.

![Functionalize all the things!](/img/functionalize_all_the_things.jpg)

## Modularity

By breaking down your analysis into functions, you end up with blocks of code
that can interact and depend on each others in explicit ways. It allows you to
avoid repeating yourself, and you will be able to re-use the functions you
create for other projects more easily than if your paper only contains scripts.

House of cards vs. house made of lego.

## Less variables to worry about to focus on the important stuff

If your report only contains scripts, you are going to accumulate many variables
in your document and you are going to have to worry about to avoid name
conflicts among all these temporary variables that store intermediate versions
of your datasets but won't need in your analysis. By putting everything into
functions, these variables will be hidden from your main environment and you can
focus on the important stuff: the inputs and the outputs of your workflow.

By writing functions to get the variables, results, or figures, you make it
clear in your analysis what is important, how your variables are related, which
dataset depend on which one, etc.

## Documenting your code

Ideally, your code should be written so that it's easy to understand and your
intentions are clear. However, what might seem clear to you now might be clear
as mud 6 months from now or even 3 weeks from now. Other times, it might not
seem very efficient to refactor a piece of code to make it clearer, and you end
up with a piece of your that works but is klunky. If you thrive on geekiness
and/or nerdiness you might endup over engineering a part of your code and make
it more difficult to understand a few weeks later. In all of these situations,
and even if you think your code is clear and simple, it's important that you
document your code and your functions, for your collaborators, and your future
self.

If all your analysis is made up of scripts, with pieces that are repeated in
multiple parts of your document, things can get out of hands pretty quickly. Not
only it is more difficult to maintain because you will have to find and replace
the thing that you need to change in multiple places of your code, but managing
documentation is also challenging. Do you also duplicate your comments where you
duplicate parts of your scripts? How do you keep the duplicated comments in
sync? Re-organizing your scripts into functions (or organizing your analysis in
functions from the beginning) will allow you to explicitly document the dataset
or the parameters on which your function, and therefore your results, depends
on.

The easiest way to document your code, is to add comments around your functions
to explicitly indicate what each function is doing, what the arguments are
supposed to be (class and format) and the kind of output you will get from it.

You may also want to take advantage of roxygen, it's a format that allows the
documentation of functions. This format is not very different from simple
comments, you just need to add some keywords to define what will end up as
different sections in what will become the help page for the function you are
writing. This is not a strict requirement, and will not make your analysis
necessarily more reproducible, but it will be useful down the road, if you think
you will convert your manuscript into a package (see aside below).

When documenting your functions, it important to not only document the kind of
input your function takes, but also the format and structure of the output.

## Testing

When you start writing a lot of code for your paper, it could become easier to
introduce bugs or that the data set doesn't include all the data it should. If
your bug breaks something in your analysis, you might be able to find it easily,
but you may also introduce "silent" bugs.

If all your code is made up of functions, then you can control the input and
test for the output. It is something that would be difficult if not impossible
to do if all your analysis is in the form of a long script.

The package testthat provides a powerful and easy to use framework to build
tests for your code.

> ### Should you convert your manuscript into a package?
>
> Pros: common format, allows to leverage the infrastructure for packages
> (tests, all functions are properly documented), can make sure it will be fully
> cross-platform Cons: no good place for the manuscript, you have to put the
> data in weird spots, you may have to dissociate code for functions and code
> for analysis Bottom line: it really depends on your type of paper, how much
> code there is in it and whether others might end up re-using it. It's not
> because your manuscript follows the conventions of a package that you need to
> submit to CRAN.

## Organizing your files

![example of file organization we will use today](img/file_organization.png)

Today we are going to work on functionalizing a knitr document that is more
complex than what we have seen so far but not quite as complex as a "real"
research document could look like.

Show the starting point:  one file per country in the data-raw folder

Explain motivation: change of life expectancy by continent

Go through details of the document.
